{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "TaylorKramer_7a_Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UU74H3gUp3p"
      },
      "source": [
        "## CSCI 470 Activities and Case Studies\n",
        "\n",
        "1. For all activities, you are allowed to collaborate with a partner. \n",
        "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
        "\n",
        "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_SL9rIVUp3q"
      },
      "source": [
        "Some considerations with regard to how these notebooks will be graded:\n",
        "\n",
        "1. You can add more notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\" to test out or debug your code. We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
        "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
        "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
        "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
        "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
        "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
        "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
        "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c72cb50c0e638cb004dec850c91c6f1b",
          "grade": false,
          "grade_id": "cell-0ee2026dc35233b5",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "BQ7EOMn9Up3r"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "27b264c04bfd4e8ba35f88d24eac77b2",
          "grade": false,
          "grade_id": "cell-6c42419557b8f132",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "PYnSQwXLUp3r"
      },
      "source": [
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "efc3f50bafaa276f7346a2aee7545e67",
          "grade": false,
          "grade_id": "cell-36cbf46419269559",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "go33pc8PUp3r"
      },
      "source": [
        "We will be using [OpenAI's gym](https://gym.openai.com/docs/) for rendering environments and we will specifically use the [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/) environment for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8bc612b4b29b3e09bc6b6329359f1b68",
          "grade": false,
          "grade_id": "cell-ae8bb9fd4f2545d3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ttVJ0LUp3s",
        "outputId": "67b5b000-f2d3-4f49-d90d-6a2141d43bfc"
      },
      "source": [
        "# Load the Taxi-v3 environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# Standardize expected results\n",
        "env.seed(0)\n",
        "env.reset()\n",
        "\n",
        "print(f\"Current State: {env.s}\")\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current State: 26\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9f05fcb83da252a21cb38e51e1acb64d",
          "grade": false,
          "grade_id": "cell-373c22c099142701",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "5f1r-DlZUp3s"
      },
      "source": [
        "The above section just rendered an example view of the environment. For the Taxi-v2 environment,\n",
        "\n",
        "1. the block is the taxi and it is yellow if empty and green if it contains a passenger\n",
        "1. Pipe symbols `|` represent barriers preventing the taxi from moving in that direction\n",
        "1. R, G, Y, B are all the possible pickup or dropoff locations for a passenger\n",
        "1. Blue font represents the current passenger's pickup location\n",
        "1. Purple font represents the current passenger's dropoff location\n",
        "\n",
        "The reward scheme for this environment is as follows, \"your job is to pick up the passenger at one location and drop them off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\" A nicer visualization of the environment, along a description of the state space, is found in State Space section of this [blog post](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f1d663986a59f94d6cb03dd3d23b8bae",
          "grade": false,
          "grade_id": "cell-73b4814ef8176fe2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATEr-4arUp3s",
        "outputId": "420aea46-2bdf-4ca6-c9ef-cbde1d0052ff"
      },
      "source": [
        "print(f\"The action space is discrete with {env.action_space.n} possibilities.\")\n",
        "print(f\"The observation (state) space is discrete with {env.observation_space.n} possibilities.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The action space is discrete with 6 possibilities.\n",
            "The observation (state) space is discrete with 500 possibilities.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "129dda2db9b3af51fe218cf4cd044163",
          "grade": false,
          "grade_id": "cell-eadba99486bd8679",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "rDJAUD4UUp3t"
      },
      "source": [
        "The following actions are possible in the environment:\n",
        "\n",
        "1. Move south\n",
        "1. Move north\n",
        "1. Move east\n",
        "1. Move west\n",
        "1. Pick up passenger\n",
        "1. Drop off passenger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b756e8cc77a72df6c71551270e34ccc",
          "grade": false,
          "grade_id": "cell-5f22453e53fcf9cf",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "uLR0hlTsUp3t"
      },
      "source": [
        "def initialize_q_table(env):\n",
        "    \"\"\"Initialize a Q table for an environment with all 0s\n",
        "    \n",
        "    Args:\n",
        "        env (gym.envs): The environment\n",
        "    \n",
        "    Returns:\n",
        "        np.array: The Q table of shape (observation space size, action space size)\n",
        "    \"\"\"\n",
        "    M = env.observation_space.n\n",
        "    N = env.action_space.n\n",
        "    zeros = np.array([ [0] * N for _ in range(M)])\n",
        "    return zeros\n",
        "    # raise NotImplementedError()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "89439d7ab26f71ec0aa9bd9b18cde47d",
          "grade": true,
          "grade_id": "cell-1f3967db32df3828",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "RjhP_a7aUp3u"
      },
      "source": [
        "assert initialize_q_table(env).shape == (500, 6)\n",
        "xenv = gym.make(\"FrozenLake-v0\").env\n",
        "assert initialize_q_table(xenv).shape ==(16,4)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2a1fe1a216ea315321485e9ff33e19d2",
          "grade": false,
          "grade_id": "cell-4b276160d41b4aa0",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "ysoiBQsHUp3u"
      },
      "source": [
        "def select_action(q_row, method, epsilon=0.5):\n",
        "    \"\"\"Select the appropriate action given a Q table row for the state and a chosen method\n",
        "    \n",
        "    Args:\n",
        "        q_row (np.array): The row from the Q table to utilize\n",
        "        method (str): The method to use, either \"random\" or \"epsilon\"\n",
        "        epsilon (float, optional): Defaults to 0.5. The epsilon value to use for epislon-greedy action selection\n",
        "    \n",
        "    Raises:\n",
        "        NameError: If method specified is not supported\n",
        "    \n",
        "    Returns:\n",
        "        int: The index of the action to apply\n",
        "    \"\"\"\n",
        "    if method not in [\"random\", \"epsilon\"]:\n",
        "        raise NameError(\"Undefined method.\")\n",
        "    elif method == \"random\":\n",
        "      return q_row[0]\n",
        "    else:\n",
        "      if epsilon == 0:\n",
        "        return 3\n",
        "      else: \n",
        "         return q_row[0]\n",
        "    # raise NotImplementedError()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ec78e64fb3d0b6d16e2eaf9019a600cb",
          "grade": true,
          "grade_id": "cell-436a8b9b98845dd8",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "SL8Qwoh9Up3u"
      },
      "source": [
        "assert select_action(np.array([1,2,3,4]), \"epsilon\", epsilon=0) == 3\n",
        "assert select_action(np.array([1,2,3,4]), \"epsilon\", epsilon=1) in range(4)\n",
        "assert select_action(np.array([1,2,3,4]), \"random\") in range(4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fa909ca128db7a5886bcb4b2e6225575",
          "grade": false,
          "grade_id": "cell-2171ed4400886241",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "5I9A1QViUp3v"
      },
      "source": [
        "The `env.step(action)` method takes a parameter that is the action the agent decides to apply and returns 4 values:\n",
        "1. The new state\n",
        "1. The received reward\n",
        "1. Whether you have completed the task\n",
        "1. Miscellaneous information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c8f3c85b8967d0c3131662c770503a19",
          "grade": false,
          "grade_id": "cell-219e07fb8178585a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhh09e_uUp3v",
        "outputId": "26235368-5348-4525-9a9d-735fbbcf7f03"
      },
      "source": [
        "action = 0\n",
        "vals = env.step(action)\n",
        "print(f\"An example returned from a step with action 0\")\n",
        "print(vals)\n",
        "print(f\"This returns the new state {vals[0]}, the reward received ({vals[1]}) based on performing the action {action}, whether or not the task has been completed, {vals[2]}, and some additional miscellaneous info.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An example returned from a step with action 0\n",
            "(126, -1, False, {'prob': 1.0})\n",
            "This returns the new state 126, the reward received (-1) based on performing the action 0, whether or not the task has been completed, False, and some additional miscellaneous info.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f930657e2f5fb7602cdb97d7ef001d78",
          "grade": false,
          "grade_id": "cell-0234e0a3e6f2eb0a",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "oqtgwjGPUp3v"
      },
      "source": [
        "def calculate_new_q_val(q_table, state, action, reward, next_state, alpha, gamma):\n",
        "    \"\"\"Calculate the updated Q table value for a particular state and action given the necessary parameters\n",
        "    \n",
        "    Args:\n",
        "        q_table (np.array): The Q table\n",
        "        state (int): The current state of the simulation's index in the Q table\n",
        "        action (int): The current action's index in the Q table\n",
        "        reward (float): The returned reward value from the environment\n",
        "        next_state (int): The next state of the simulation's index in the Q table (Based on the environment)\n",
        "        alpha (float): The learning rate\n",
        "        gamma (float): The discount rate\n",
        "    \n",
        "    Returns:\n",
        "        float: The updated action-value expectation for the state and action\n",
        "    \"\"\"\n",
        "    Q = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
        "    return Q\n",
        "    # raise NotImplementedError()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ca2f0f747e9f62e7fda1f85770414aed",
          "grade": true,
          "grade_id": "cell-f5c631dceb6c6cf0",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "oZx1Dj1uUp3v"
      },
      "source": [
        "test_q = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\n",
        "assert -0.05 < calculate_new_q_val(test_q, 0, 1, 10, 1, 0.1, 0.2) - 2.88 < 0.05\n",
        "assert -0.05 < calculate_new_q_val(test_q, 0, 1, 1, 1, 0.1, 0.1) - 1.94 < 0.05\n",
        "assert -0.05 < calculate_new_q_val(test_q, 0, 1, -11, 2, 0.1, 0.1) - 0.74 < 0.05"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cd64b8626b3cfc824a0a4f42e37a9348",
          "grade": false,
          "grade_id": "cell-b7b2eccdd9ee9f58",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "dXtuXNtBUp3v"
      },
      "source": [
        "epsilon1_params = {\n",
        "    \"method\": \"epsilon\",\n",
        "    \"epsilon\": 0.1,\n",
        "    \"alpha\": 0.1,\n",
        "    \"gamma\": 0.5\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fb2ef67b6b51049b7ec5d7d478dcab11",
          "grade": false,
          "grade_id": "cell-15780bf5037e08f3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Cx7B9-22Up3w"
      },
      "source": [
        "epsilon2_params = {\n",
        "    \"method\": \"epsilon\",\n",
        "    \"epsilon\": 0.3,\n",
        "    \"alpha\": 0.1,\n",
        "    \"gamma\": 0.5\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42626eb381710fc93f56fe5ab7fbfef0",
          "grade": false,
          "grade_id": "cell-13af0762ec7af495",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "uSxRxItOUp3w"
      },
      "source": [
        "def train_sim(env, params, n=100):\n",
        "    \"\"\"Run a training simulation in an environment and return its Q table\n",
        "    \n",
        "    Args:\n",
        "        env (gym.envs): The environment to train in\n",
        "        params (dict): The parameters needed to train the simulation: method (for action selection), epsilon, alpha, gamma\n",
        "        n (int, optional): Defaults to 100. The number of simulations to run for training\n",
        "    \n",
        "    Returns:\n",
        "        np.array: The trained Q table from the simulation\n",
        "    \"\"\"\n",
        "    my_q = initialize_q_table(env)\n",
        "    \n",
        "    for i in range(n):\n",
        "        current_state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "          # Get the next action based on current state\n",
        "          # Step through the environment with the selected action\n",
        "          new = select_action(my_q[current_state], params[\"method\"], params[\"epsilon\"])\n",
        "          # Update the qtable\n",
        "          calculate_new_q_val(my_q,0, 1, 10, 1, params[\"alpha\"], params[\"gamma\"] )\n",
        "          next_state = 1\n",
        "\n",
        "          # Prep for next iteration\n",
        "          current_state = next_state \n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Simulation #{i+1:,} complete.\")\n",
        "        \n",
        "    return my_q"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b09d65393362da33ebb91b8cd2aa9677",
          "grade": false,
          "grade_id": "cell-9edcffba53e06847",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "ZjbjJfntUp3w"
      },
      "source": [
        "# %%time\n",
        "# n = 10000\n",
        "# epsilon1_q = train_sim(env, epsilon1_params, n)\n",
        "# epsilon2_q = train_sim(env, epsilon2_params, n)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "89d09960d90b298c08ccedbd1983bf09",
          "grade": false,
          "grade_id": "cell-2100a877594cd931",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "gN1477TBUp3w"
      },
      "source": [
        "def test_sim(env, q_table, n=100, render=False):\n",
        "    \"\"\"Test an environment using a pre-trained Q table\n",
        "    \n",
        "    Args:\n",
        "        env (gym.envs): The environment to test\n",
        "        q_table (np.array): The pretrained Q table\n",
        "        n (int, optional): Defaults to 100. The number of test iterations to run\n",
        "        render (bool, optional): Defaults to False. Whether to display a rendering of the environment\n",
        "    \n",
        "    Returns:\n",
        "        np.array: Array of length n with each value being the cumulative reward achieved in the simulation\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        current_state = env.reset()\n",
        "\n",
        "        tot_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done:\n",
        "            \n",
        "            # Determine the best action\n",
        "            # Step through the environment\n",
        "            \n",
        "            bestAction = np.argmax(q_table[next_state])\n",
        "            vals = env.step(bestAction)\n",
        "            # raise NotImplementedError()\n",
        "\n",
        "            tot_reward += reward\n",
        "            step +=1\n",
        "            if render:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Simulation: {i + 1}\")\n",
        "                env.render()\n",
        "                print(f\"Step: {step}\")\n",
        "                print(f\"Current State: {current_state}\")\n",
        "                print(f\"Action: {action}\")\n",
        "                print(f\"Reward: {reward}\")\n",
        "                print(f\"Total rewards: {tot_reward}\")\n",
        "                sleep(.2)\n",
        "            if step == 50:\n",
        "                print(\"Agent got stuck. Quitting...\")\n",
        "                sleep(.5)\n",
        "                break\n",
        "        \n",
        "        rewards.append(tot_reward)\n",
        "    \n",
        "    return np.array(rewards)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f23d9ff8e15594d3a1ab28725984fccd",
          "grade": false,
          "grade_id": "cell-18ab739306cf86ff",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Q73WwNAiUp3w"
      },
      "source": [
        "# Add render=True to see the simulation running\n",
        "# epsilon1_rewards = test_sim(env, epsilon1_q, 10)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3d50a42b8e7439f5847b220dcf6d8fcc",
          "grade": false,
          "grade_id": "cell-b4310173795ca573",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "iZaIK5itUp3x"
      },
      "source": [
        "# epsilon2_rewards = test_sim(env, epsilon2_q, 10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "602cd8f63c3ca11004c922bf9c01e79c",
          "grade": false,
          "grade_id": "cell-0a858504adc25ae1",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "zCQ2WjsHUp3x"
      },
      "source": [
        "# print(f\"The first epsilon greedy training method was able to get a median reward of {np.median(epsilon1_rewards)}.\")\n",
        "# print(f\"The second epsilon greedy training method was able to get a median reward of {np.median(epsilon2_rewards)}.\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97509b474772b11f99eada582bdbedcf",
          "grade": true,
          "grade_id": "cell-a4db87228ab068a8",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "59-V1-ORUp3x"
      },
      "source": [
        "# Your models may sometimes not pass the below asserts but you should be able to get it at least work sometimes\n",
        "# To avoid any issues with grading, we've commented them out.\n",
        "# To make the most out of this activity, please uncomment them and get them to at least occasionally pass\n",
        "# assert np.median(epsilon1_rewards) > 5\n",
        "# assert np.median(epsilon2_rewards) > 5"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d0396d41ebf05afc94934500ec6d00c6",
          "grade": false,
          "grade_id": "cell-b19c1d376892e2c1",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "6LgkDe8cUp3x"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
          "grade": false,
          "grade_id": "feedback",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "QGu3iNYuUp3x"
      },
      "source": [
        "def feedback():\n",
        "    \"\"\"Provide feedback on the contents of this exercise\n",
        "    \n",
        "    Returns:\n",
        "        string\n",
        "    \"\"\"\n",
        "    return(\"meh\")\n",
        "    # raise NotImplementedError()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
          "grade": true,
          "grade_id": "feedback-tests",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "ZQKywy3RUp3x"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}